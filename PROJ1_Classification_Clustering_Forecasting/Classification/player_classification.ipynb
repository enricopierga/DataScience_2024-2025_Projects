{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolare vittorie su singola superficie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surface_wins(df, players_df):\n",
    "    surface_wins = df.groupby(['Winner', 'Surface']).size().unstack(fill_value=0)\n",
    "    surface_wins.columns = [f\"{surface}_wins\" for surface in surface_wins.columns]\n",
    "    return players_df.merge(surface_wins, how='left', left_on='Player_name', right_index=True).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcolare vittorie indoor e outdoor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_court_wins(df, players_df):\n",
    "    court_wins = df.groupby(['Winner', 'Court']).size().unstack(fill_value=0)\n",
    "    court_wins.columns = [f\"{court}_wins\" for court in court_wins.columns]\n",
    "    return players_df.merge(court_wins, how='left', left_on='Player_name', right_index=True).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_surface_ratios(players_df):\n",
    "    \"\"\"\n",
    "    Aggiunge le feature di ratio per ogni superficie al DataFrame.\n",
    "    Ogni ratio è il rapporto tra i match vinti e i match giocati per la superficie,\n",
    "    con gestione dei casi in cui il denominatore è 0.\n",
    "    \"\"\"\n",
    "    # Evita divisioni per 0 con un controllo diretto\n",
    "    players_df['Clay_ratio'] = players_df['Clay_wins'] / players_df['Clay_matches'].replace(0, 1)\n",
    "    players_df['Hard_ratio'] = players_df['Hard_wins'] / players_df['Hard_matches'].replace(0, 1)\n",
    "    players_df['Grass_ratio'] = players_df['Grass_wins'] / players_df['Grass_matches'].replace(0, 1)\n",
    "\n",
    "    # Gestione NaN o inf residui\n",
    "    players_df.fillna(0, inplace=True)\n",
    "\n",
    "    return players_df\n",
    "\n",
    "#TODO: aggiungi soglia di almeno tot. partite, sennò butta la riga del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punti totali accumulati su ogni superficie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surface_points(df, players_df):\n",
    "    # Definizione dei punti in base al tipo di torneo e al round\n",
    "    points_table = {\n",
    "        \"Grand Slam\": {\n",
    "            \"Winner\": 2000,\n",
    "            \"The Final\": 1200,\n",
    "            \"Semifinals\": 720,\n",
    "            \"Quarterfinals\": 360,\n",
    "            \"2nd Round\": 180,\n",
    "            \"1st Round\": 10,\n",
    "        },\n",
    "        \"Masters 1000\": {\n",
    "            \"Winner\": 1000,\n",
    "            \"The Final\": 600,\n",
    "            \"Semifinals\": 360,\n",
    "            \"Quarterfinals\": 180,\n",
    "            \"2nd Round\": 90,\n",
    "            \"1st Round\": 10,\n",
    "        },\n",
    "        \"ATP500\": {\n",
    "            \"Winner\": 500,\n",
    "            \"The Final\": 300,\n",
    "            \"Semifinals\": 180,\n",
    "            \"Quarterfinals\": 90,\n",
    "            \"2nd Round\": 45,\n",
    "            \"1st Round\": 10,\n",
    "        },\n",
    "        \"ATP250\": {\n",
    "            \"Winner\": 250,\n",
    "            \"The Final\": 150,\n",
    "            \"Semifinals\": 90,\n",
    "            \"Quarterfinals\": 45,\n",
    "            \"2nd Round\": 20,\n",
    "            \"1st Round\": 5,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Calcola i punti per ogni match basandosi sul tipo di torneo e sul round\n",
    "    def get_points(row):\n",
    "        tournament_type = row['Series']\n",
    "        round_type = row['Round']\n",
    "        return points_table.get(tournament_type, {}).get(round_type, 0)\n",
    "\n",
    "    # Applica la funzione per calcolare i punti per ogni riga\n",
    "    df['Round_points'] = df.apply(get_points, axis=1)\n",
    "\n",
    "    # Calcola i punti totali guadagnati su ciascuna superficie\n",
    "    surface_points = df.groupby(['Winner', 'Surface'])['Round_points'].sum().unstack(fill_value=0)\n",
    "    surface_points.columns = [f\"{surface}_points\" for surface in surface_points.columns]\n",
    "\n",
    "    # Merge con il DataFrame dei giocatori\n",
    "    players_df = players_df.merge(surface_points, how='left', left_on='Player_name', right_index=True).fillna(0)\n",
    "\n",
    "    return players_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numero di Match Giocati per Superficie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_surface_matches(df, players_df):\n",
    "    surface_matches = df.groupby(['P1_name', 'Surface']).size().unstack(fill_value=0)\n",
    "    surface_matches.columns = [f\"{surface}_matches\" for surface in surface_matches.columns]\n",
    "    return players_df.merge(surface_matches, how='left', left_on='Player_name', right_index=True).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anni di Attività"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_years_active(df, players_df):\n",
    "    years_active = df.groupby('P1_name')['Year'].nunique().rename('Years_active')\n",
    "    return players_df.merge(years_active, how='left', left_on='Player_name', right_index=True).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Calcolo superficie preferita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_preferred_surface(players_df):\n",
    "    # Somma normalizzata per combinare vittorie e punti\n",
    "    players_df['Clay_score'] = players_df['Clay_wins'] + players_df['Clay_points'] / 250\n",
    "    players_df['Hard_score'] = players_df['Hard_wins'] + players_df['Hard_points'] / 250\n",
    "    players_df['Grass_score'] = players_df['Grass_wins'] + players_df['Grass_points'] / 250\n",
    "\n",
    "    # Determina la superficie preferita\n",
    "    players_df['Preferred_surface'] = players_df[['Clay_score', 'Hard_score', 'Grass_score']].idxmax(axis=1)\n",
    "    players_df['Preferred_surface'] = players_df['Preferred_surface'].str.replace('_score', '')\n",
    "\n",
    "    # Rimuove colonne temporanee\n",
    "    players_df.drop(columns=['Clay_score', 'Hard_score', 'Grass_score'], inplace=True)\n",
    "    #TODO: Spiegare bene cosa intendi per preferita\n",
    "    return players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Genero il df finale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features_and_target(df, players_df):\n",
    "    # Calcola le feature\n",
    "    players_df = calculate_surface_wins(df, players_df)\n",
    "    players_df = calculate_court_wins(df, players_df)\n",
    "    players_df = calculate_surface_points(df, players_df)\n",
    "    players_df = calculate_surface_matches(df, players_df)\n",
    "    players_df = calculate_years_active(df, players_df)\n",
    "    \n",
    "    # Determina la superficie preferita\n",
    "    players_df = calculate_preferred_surface(players_df)\n",
    "    \n",
    "    print(f\"[DEBUG] Dataframe finale con feature: {players_df.shape[0]} righe, {players_df.shape[1]} colonne\")\n",
    "    print(players_df.head(50))\n",
    "    return players_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carico dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il dataset e crea il dataframe che utilizzo per l'addestramento\n",
    "print(\"Caricamento del dataset\")\n",
    "df = pd.read_csv('./atp_tennis.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esplora le prime righe del dataset\n",
    "print(\"Prime righe del dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Mostra informazioni sulle colonne\n",
    "print(\"\\nInformazioni sul dataset:\")\n",
    "print(df.info())\n",
    "\n",
    "# Conta i valori nulli in ciascuna colonna\n",
    "print(\"\\nValori nulli per colonna:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulizia valori vuoti e filtraggio, aggiunta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converti la colonna Date in formato datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
    "# Aggiungi la colonna Year estraendola dalla colonna Date\n",
    "df['Year'] = df['Date'].dt.year\n",
    "# Filtra solo gli ultimi 5 anni\n",
    "latest_year = df['Year'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostra le righe totali prima del filtraggio\n",
    "print(\"\\nPrima del filtraggio e pulizia:\")\n",
    "print(f\"Righe totali: {len(df)}\")\n",
    "\n",
    "# Filtra le quote negative o nulle\n",
    "df = df[(df['Odd_1'] > 0) & (df['Odd_2'] > 0)]\n",
    "# Filtra i punti ATP positivi\n",
    "df = df[(df['Pts_1'] > 0) & (df['Pts_2'] > 0)]\n",
    "\n",
    "# Rimuovi righe con valori NaN in qualsiasi colonna\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "print(f\"Dati filtrati per gli ultimi 5 anni: {df_recent.shape}\")\n",
    "\n",
    "\n",
    "# Rinominare le colonne\n",
    "df_recent.rename(\n",
    "    columns={\n",
    "        'Pts_1': 'ATP_pts_p1',\n",
    "        'Pts_2': 'ATP_pts_p2',\n",
    "        'Rank_1': 'ATP_rank_p1',\n",
    "        'Rank_2': 'ATP_rank_p2',\n",
    "        'Odd_1': 'PreMatch_Odd_p1',\n",
    "        'Odd_2': 'PreMatch_Odd_p2',\n",
    "        'Player_1': 'P1_name',\n",
    "        'Player_2': 'P2_name',\n",
    "        'Best of': 'N_sets'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Calcola la differenza di ranking per ogni match\n",
    "df_recent['Rank_diff'] = abs(df_recent['ATP_rank_p1'] - df_recent['ATP_rank_p2'])\n",
    "\n",
    "# Stampa le righe totali dopo il filtraggio\n",
    "print(\"\\nDopo il filtraggio:\")\n",
    "print(f\"Righe rimanenti: {len(df_recent)}\")\n",
    "\n",
    "# Stampa il numero di valori mancanti per ogni colonna (dopo il filtraggio, dovrebbe essere 0)\n",
    "print(\"\\nValori nulli per colonna (dopo il filtraggio):\")\n",
    "print(df_recent.isnull().sum())\n",
    "\n",
    "# Visualizza le prime righe del DataFrame pulito\n",
    "print(\"\\nPrime righe del DataFrame pulito:\")\n",
    "print(df_recent.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo dataframe dei giocatori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estrai tutti i giocatori da P1_name e P2_name\n",
    "all_players = pd.concat([df_recent['P1_name'], df_recent['P2_name']])\n",
    "\n",
    "# Rimuovi i duplicati e crea un set unico di giocatori\n",
    "unique_players = sorted(all_players.unique())\n",
    "\n",
    "# Crea un nuovo dataframe dei giocatori\n",
    "players_df = pd.DataFrame(unique_players, columns=['Player_name'])\n",
    "\n",
    "# Verifica il dataframe creato\n",
    "print(f\"Dataframe dei giocatori creato con {players_df.shape[0]} giocatori unici:\")\n",
    "print(players_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_df_final = generate_features_and_target(df_recent, players_df)\n",
    "\n",
    "# Filtra i giocatori con almeno 30 partite giocate tra Clay, Hard o Grass\n",
    "players_df_filtered = players_df_final[\n",
    "    (players_df_final['Clay_matches'] + players_df_final['Hard_matches'] + players_df_final['Grass_matches']) >= 10\n",
    "]\n",
    "\n",
    "# Visualizza il numero di giocatori filtrati e le prime righe del nuovo DataFrame\n",
    "print(f\"\\n[DEBUG] Numero di giocatori filtrati: {len(players_df_filtered)}\")\n",
    "print(players_df_filtered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica la funzione al DataFrame\n",
    "players_df_filtered = add_surface_ratios(players_df_filtered)\n",
    "\n",
    "# Controlla il risultato\n",
    "print(\"\\n[DEBUG] Prime righe del DataFrame con i ratio aggiunti:\")\n",
    "print(players_df_filtered[['Player_name', 'Clay_ratio', 'Hard_ratio', 'Grass_ratio']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappatura della colonna Preferred_surface\n",
    "surface_mapping = {'Clay': 0, 'Hard': 1, 'Grass': 2}\n",
    "players_df_filtered['Preferred_surface_mapped'] = players_df_filtered['Preferred_surface'].map(surface_mapping)\n",
    "\n",
    "print(\"\\n[DEBUG] Colonna di target mappata:\")\n",
    "print(players_df_filtered[['Preferred_surface', 'Preferred_surface_mapped']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Feature selezionate per ridurre la ridondanza\n",
    "selected_features = [\n",
    "    'Clay_ratio', 'Hard_ratio', 'Grass_ratio', 'Clay_wins', 'Grass_wins', 'Hard_wins',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creazione del DataFrame X e del target\n",
    "X = players_df_filtered[selected_features]\n",
    "y = players_df_filtered['Preferred_surface_mapped']\n",
    "\n",
    "# Suddivisione train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"[DEBUG] Dimensione train set: {X_train.shape}, Dimensione test set: {X_test.shape}\")\n",
    "\n",
    "# Verifica del bilanciamento iniziale\n",
    "print(\"Distribuzione dei target nel train set prima del bilanciamento:\", Counter(y_train))\n",
    "print(\"Distribuzione dei target nel test set:\", Counter(y_test))\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Bilanciamento del test set con oversampling semplice\n",
    "def balance_test_set(X_test, y_test, desired_samples=100):\n",
    "    test_data = X_test.copy()\n",
    "    test_data['target'] = y_test\n",
    "    balanced_test_data = []\n",
    "\n",
    "    for target in test_data['target'].unique():\n",
    "        target_data = test_data[test_data['target'] == target]\n",
    "        upsampled_data = resample(\n",
    "            target_data,\n",
    "            replace=True,  # Oversampling con duplicazione\n",
    "            n_samples=desired_samples,\n",
    "            random_state=42\n",
    "        )\n",
    "        balanced_test_data.append(upsampled_data)\n",
    "\n",
    "    final_test_data = pd.concat(balanced_test_data)\n",
    "    X_test_balanced = final_test_data.drop('target', axis=1)\n",
    "    y_test_balanced = final_test_data['target']\n",
    "    return X_test_balanced, y_test_balanced\n",
    "\n",
    "# Applica il bilanciamento al test set\n",
    "X_test_balanced, y_test_balanced = balance_test_set(X_test, y_test, desired_samples=50)\n",
    "print(\"Distribuzione del test set dopo il bilanciamento:\", Counter(y_test_balanced))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Identifica la classe minoritaria\n",
    "class_counts = Counter(y_train)\n",
    "minority_class = min(class_counts, key=class_counts.get)\n",
    "print(f\"Classe minoritaria identificata: {minority_class}\")\n",
    "\n",
    "# Identifica la classe minoritaria e il numero massimo desiderato\n",
    "desired_samples_per_class = 1000  # Numero di campioni desiderati per ogni classe\n",
    "desired_samples_per_class_test = 250\n",
    "\n",
    "# Configura SMOTE per aumentare a 1000 campioni per ogni classe\n",
    "smote = SMOTE(sampling_strategy={label: desired_samples_per_class for label in Counter(y_train).keys()},\n",
    "              random_state=42)\n",
    "\n",
    "# Applica SMOTE al training set\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "smote_test = SMOTE(sampling_strategy={label: desired_samples_per_class_test for label in Counter(y_test_balanced).keys()},\n",
    "              random_state=42)\n",
    "X_test_smote, y_test_smote = smote_test.fit_resample(X_test_balanced, y_test_balanced)\n",
    "\n",
    "# Controllo del bilanciamento nel test set\n",
    "print(\"Distribuzione dei target nel test set (non modificato):\", Counter(y_test))\n",
    "# Verifica della distribuzione dopo il bilanciamento\n",
    "print(\"Distribuzione dei target nel test set dopo SMOTE:\", Counter(y_test_smote))\n",
    "\n",
    "\n",
    "# Verifica del bilanciamento dopo SMOTE\n",
    "print(\"Distribuzione dei target nel train set dopo SMOTE:\", Counter(y_train_smote))\n",
    "\n",
    "\n",
    "# OPTIONAL: Codice per normalizzare/scalare i dati se necessario\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test_smote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calcola la matrice di correlazione\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "# Visualizza la matrice di correlazione\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Matrice di Correlazione delle Feature\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dizionario dei modelli da testare\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Dizionario per memorizzare i risultati\n",
    "results = {}\n",
    "\n",
    "# Addestramento e valutazione di ciascun modello\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    \n",
    "    # Addestramento del modello\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    \n",
    "    # Predizione\n",
    "    y_pred = model.predict(X_test_smote)\n",
    "    \n",
    "    # Report di classificazione\n",
    "    acc = accuracy_score(y_test_smote, y_pred)\n",
    "    print(f\"Accuracy: {acc:.2f}\")\n",
    "    print(classification_report(y_test_smote, y_pred, target_names=surface_mapping.keys()))\n",
    "    \n",
    "    # Matrice di confusione\n",
    "    conf_matrix = confusion_matrix(y_test_smote, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=surface_mapping.keys(), yticklabels=surface_mapping.keys())\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Salva i risultati\n",
    "    results[model_name] = {\n",
    "        \"accuracy\": acc,\n",
    "        \"classification_report\": classification_report(y_test_smote, y_pred, target_names=surface_mapping.keys(), output_dict=True),\n",
    "        \"conf_matrix\": conf_matrix\n",
    "    }\n",
    "\n",
    "# Stampa dei risultati finali\n",
    "print(\"\\n--- Riepilogo Risultati ---\")\n",
    "for model_name, result in results.items():\n",
    "    print(f\"{model_name}: Accuracy = {result['accuracy']:.2f}\")\n",
    "    \n",
    "#TODO: stampare curve validazione\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Supponendo che y_test e y_pred_proba siano definiti:\n",
    "# - y_test: target originali\n",
    "# - y_pred_proba: probabilità predette per ciascuna classe\n",
    "\n",
    "# Binarizzazione dei target per la gestione multiclasse\n",
    "n_classes = len(np.unique(y_test_smote))\n",
    "y_test_binarized = label_binarize(y_test_smote, classes=np.arange(n_classes))\n",
    "\n",
    "# Calcolo della curva ROC e AUC per ogni classe\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], model.predict_proba(X_test_smote)[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Calcolo delle curve micro-average e macro-average\n",
    "# Micro-average: considera tutti i punti come un unico problema binario\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_binarized.ravel(), model.predict_proba(X_test_smote).ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Macro-average: media delle AUC di tutte le classi\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot delle curve ROC\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Curve per ogni classe\n",
    "colors = ['blue', 'green', 'red']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'Classe {i} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "# Curve macro e micro average\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.2f})', color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label=f'Macro-average (AUC = {roc_auc[\"macro\"]:.2f})', color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "# Linea casuale\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', lw=2)\n",
    "\n",
    "# Etichette e legenda\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curva ROC Multiclasse')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
